{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fzgyl0NL4JmK"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/liverpool-ion-switching/overview\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xtmgmONp4Jmb"
   },
   "outputs": [],
   "source": [
    "# dir_path = '/content/drive/My Drive/Colab Notebooks/liverpool_ion_channels/may_21_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0g5-izRI4Jmz",
    "outputId": "7a0d8026-cb97-4b4f-de41-89d1a331d78f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gGLN4Iw24JnH"
   },
   "outputs": [],
   "source": [
    "# DataLoader and Dataset using torch's functions\n",
    "# torch.float64 is Double. torch.float32 is Float\n",
    "class IonDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_file, seq_dim=10, for_training=True, train_avg=0, train_std=1):\n",
    "        self.samples = pd.read_csv(csv_file)\n",
    "        self.samples['observation_batch'] = self.samples['time'].apply(lambda x: math.trunc((x-.0001)/50))\n",
    "        self.samples['new_time'] = self.samples['time'].apply(lambda x: (x-.0001)%50+.0001)\n",
    "        self.seq_dim = seq_dim\n",
    "        self.for_training = for_training\n",
    "        self.num_of_observations = self.samples['observation_batch'].max()\n",
    "        self.categories = self.getcategories()\n",
    "        \n",
    "        if for_training:\n",
    "            self.avg = self.getsignalavg()\n",
    "            self.std = self.getsignalstd()\n",
    "        else:\n",
    "            self.avg = train_avg\n",
    "            self.std = train_std\n",
    "        self.samples['normalized_signal'] = (self.samples['signal'] - self.avg) / self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        observation = self.samples.iloc[idx]\n",
    "        obs_batch = observation['observation_batch']\n",
    "        \n",
    "        observation_slice = self.samples[max(idx-self.seq_dim+1, 0):idx+1]\n",
    "        observation_slice = observation_slice[observation_slice.observation_batch == obs_batch]\n",
    "        \n",
    "        \n",
    "        sample = observation_slice.loc[:, ('normalized_signal')]\n",
    "        sample = torch.tensor(sample.values, dtype=torch.float32).unsqueeze(1)\n",
    "        try:\n",
    "            assert sample.shape[0] == self.seq_dim\n",
    "        except AssertionError as error:\n",
    "            missing = self.seq_dim-sample.shape[0]\n",
    "            fillers = torch.zeros([missing, sample.shape[1]], dtype=torch.float32)\n",
    "            sample = torch.cat((fillers, sample), 0)\n",
    "            \n",
    "        if self.for_training:\n",
    "            channel = torch.tensor(observation['open_channels'], dtype=torch.long)\n",
    "            return sample, channel\n",
    "        else:\n",
    "            return sample\n",
    "    \n",
    "    \n",
    "    def getcategories(self):\n",
    "        try:\n",
    "            categories = np.sort(self.samples.open_channels.unique())\n",
    "            return torch.from_numpy(categories).to(device)\n",
    "        except Exception:\n",
    "            return None\n",
    "    \n",
    "    def getsignalavg(self):\n",
    "        return np.mean(self.samples['signal'])\n",
    "    \n",
    "    def getsignalstd(self):\n",
    "        return np.std(self.samples['signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0gECvqaY4JnQ"
   },
   "outputs": [],
   "source": [
    "# num_of_timesteps\n",
    "seq_dim = 500\n",
    "\n",
    "# the data used for train and test where taken from: https://www.kaggle.com/cdeotte/one-feature-model-0-930/output\n",
    "train_val_dataset = IonDataset(\"/kaggle/input/no-drift-data/kaggle_data/train_no_drift.csv\", seq_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JSNjU1Ca4Jna",
    "outputId": "abad7f33-d482-4331-fccb-bb7dcaf5eb5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, n = train_val_dataset[0]\n",
    "# m = train_dataset[7230]\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "117mmytQ4Jnk",
    "outputId": "05de0e9c-6418-4c1c-da5f-b0ddb2016aec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sAooa_nf4Jn1"
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = torch.utils.data.random_split(train_val_dataset, [4000000, 1000000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lpHgqnQU4Jn8"
   },
   "outputs": [],
   "source": [
    "# m_t, n_t = train_dataset[7229]\n",
    "# m_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GWKdxdSk4JoB"
   },
   "outputs": [],
   "source": [
    "# len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ivmKNsiw4JoH"
   },
   "outputs": [],
   "source": [
    "# m_v, n_v = val_dataset[7229]\n",
    "# m_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "n9ENQ5Fa4JoU"
   },
   "outputs": [],
   "source": [
    "# len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "c73SbGbN4Joa",
    "outputId": "211d77bd-dc87-46ca-f841-22ea22766206"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = train_val_dataset.categories\n",
    "n_categories = len(categories)\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tfs0XgiD4Joe",
    "outputId": "07b914d5-81d2-4329-a90b-7a5849dbaf37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fab018677d0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_val_dataset.samples['open_channels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "W_8xOeK84Jo8"
   },
   "outputs": [],
   "source": [
    "#train_val_dataset.samples.groupby(['observation_batch', 'open_channels']).agg({'open_channels': ['count','max']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9kLg_HLV4JpB",
    "outputId": "6a04a224-0f06-4cea-e4ec-735d5a88fab5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(1, 500, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=500, out_features=11, bias=True)\n",
       "  (linear): Linear(in_features=500, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, dtype=torch.float32).to(device)\n",
    "        \n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # print(x.float().dtype)\n",
    "        # print(h0.float().dtype)\n",
    "        \n",
    "        # out, hn = self.rnn(x.float(), h0.float().detach())\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        \n",
    "        \"\"\"\n",
    "        print('out')\n",
    "        print(out.shape)\n",
    "        print(out)\n",
    "        print('hn')\n",
    "        print(hn.shape)\n",
    "        print(hn)\n",
    "        print('cn')\n",
    "        print(cn.shape)\n",
    "        print(cn)\n",
    "        \"\"\"\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        # out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "input_dim = 1\n",
    "hidden_dim = seq_dim\n",
    "layer_dim = 3\n",
    "output_dim = n_categories\n",
    "\n",
    "model = LSTM(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FZEkT-kQ4JpJ",
    "outputId": "cc32f25b-a94a-4b42-b41b-bc109042f37c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(1, 500, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=500, out_features=11, bias=True)\n",
      "  (linear): Linear(in_features=500, out_features=11, bias=True)\n",
      ")\n",
      "16\n",
      "torch.Size([2000, 1])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000])\n",
      "torch.Size([11, 500])\n",
      "torch.Size([11])\n",
      "torch.Size([11, 500])\n",
      "torch.Size([11])\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(len(list(model.parameters())))\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jNA_syVU4JpQ"
   },
   "outputs": [],
   "source": [
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "lr_uuSUF4JpX"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_iters = 3000\n",
    "# num_epochs = int(n_iters / (len(train_dataset) / batch_size))\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tsrJsOtY4Jpd"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Lqozk0G64Jpk",
    "outputId": "66524a10-9428-4a5b-d4ae-4f46df85b3ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428398394/work/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. Loss: 2.3929107189178467. Rolling Loss Avg: 2.3929107189178467. Rolling Val Loss Avg: 2.3930550813674927. Elapsed Time: 0m 1s. accuracy: 25\n",
      "Iteration: 500. Loss: 2.179532766342163. Rolling Loss Avg: 2.2597529833426258. Rolling Val Loss Avg: 2.1666088545763933. Elapsed Time: 1m 40s. accuracy: 23\n",
      "Iteration: 1000. Loss: 2.1046433448791504. Rolling Loss Avg: 2.192660999583912. Rolling Val Loss Avg: 2.0897248321109347. Elapsed Time: 3m 19s. accuracy: 24\n",
      "Iteration: 1500. Loss: 2.11738920211792. Rolling Loss Avg: 2.1461133933083207. Rolling Val Loss Avg: 1.9881485965516832. Elapsed Time: 4m 58s. accuracy: 23\n",
      "Iteration: 2000. Loss: 1.4975169897079468. Rolling Loss Avg: 2.0228564907824143. Rolling Val Loss Avg: 1.4170774707087763. Elapsed Time: 6m 36s. accuracy: 16\n",
      "Iteration: 2500. Loss: 1.404524564743042. Rolling Loss Avg: 1.8966603375396363. Rolling Val Loss Avg: 1.37523822872727. Elapsed Time: 8m 15s. accuracy: 14\n",
      "Iteration: 3000. Loss: 1.343395709991455. Rolling Loss Avg: 1.7889053860492445. Rolling Val Loss Avg: 1.1236776621253401. Elapsed Time: 9m 53s. accuracy: 13\n",
      "Iteration: 3500. Loss: 0.8946802020072937. Rolling Loss Avg: 1.690747745052061. Rolling Val Loss Avg: 1.049490718929856. Elapsed Time: 11m 31s. accuracy: 17\n",
      "Iteration: 4000. Loss: 0.7529577016830444. Rolling Loss Avg: 1.6099561368099184. Rolling Val Loss Avg: 1.056628734977157. Elapsed Time: 13m 10s. accuracy: 15\n",
      "Iteration: 4500. Loss: 1.0416330099105835. Rolling Loss Avg: 1.5472183543903195. Rolling Val Loss Avg: 1.0166859737148992. Elapsed Time: 14m 49s. accuracy: 12\n",
      "Iteration: 5000. Loss: 0.952254593372345. Rolling Loss Avg: 1.495954604094993. Rolling Val Loss Avg: 1.0019909253826849. Elapsed Time: 16m 29s. accuracy: 14\n",
      "Iteration: 5500. Loss: 0.9423344135284424. Rolling Loss Avg: 1.4518443645184484. Rolling Val Loss Avg: 0.9885677319985849. Elapsed Time: 18m 10s. accuracy: 11\n",
      "Iteration: 6000. Loss: 0.9448118805885315. Rolling Loss Avg: 1.4254817226701448. Rolling Val Loss Avg: 1.0624703257172197. Elapsed Time: 19m 50s. accuracy: 18\n",
      "Iteration: 6500. Loss: 1.1924431324005127. Rolling Loss Avg: 1.3993681245694765. Rolling Val Loss Avg: 1.0287728331707142. Elapsed Time: 21m 30s. accuracy: 15\n",
      "Iteration: 7000. Loss: 0.8966389894485474. Rolling Loss Avg: 1.372256584417614. Rolling Val Loss Avg: 1.0738291828720659. Elapsed Time: 23m 11s. accuracy: 17\n",
      "Iteration: 7500. Loss: 0.6794359683990479. Rolling Loss Avg: 1.3484752021929596. Rolling Val Loss Avg: 0.9830321647502758. Elapsed Time: 24m 51s. accuracy: 15\n",
      "Iteration: 8000. Loss: 0.7494035363197327. Rolling Loss Avg: 1.3257590876066927. Rolling Val Loss Avg: 0.9462330054353785. Elapsed Time: 26m 31s. accuracy: 16\n",
      "Iteration: 8500. Loss: 0.9283126592636108. Rolling Loss Avg: 1.3066567225766705. Rolling Val Loss Avg: 0.947938292114823. Elapsed Time: 28m 12s. accuracy: 13\n",
      "Iteration: 9000. Loss: 1.159940481185913. Rolling Loss Avg: 1.2877293700093018. Rolling Val Loss Avg: 0.9610203637017144. Elapsed Time: 29m 52s. accuracy: 12\n",
      "Iteration: 9500. Loss: 0.9108256101608276. Rolling Loss Avg: 1.2769840239455934. Rolling Val Loss Avg: 1.040592560061702. Elapsed Time: 31m 33s. accuracy: 13\n",
      "Iteration: 10000. Loss: 1.0394439697265625. Rolling Loss Avg: 1.2620508469512326. Rolling Val Loss Avg: 1.008830432538633. Elapsed Time: 33m 13s. accuracy: 13\n",
      "Iteration: 10500. Loss: 0.784200131893158. Rolling Loss Avg: 1.2481981364417878. Rolling Val Loss Avg: 0.9039635172596684. Elapsed Time: 34m 53s. accuracy: 14\n",
      "Iteration: 11000. Loss: 0.916240930557251. Rolling Loss Avg: 1.2366495832005757. Rolling Val Loss Avg: 0.9979547725783454. Elapsed Time: 36m 34s. accuracy: 17\n",
      "Iteration: 11500. Loss: 1.2211092710494995. Rolling Loss Avg: 1.2246375059591212. Rolling Val Loss Avg: 0.9834313988685608. Elapsed Time: 38m 14s. accuracy: 13\n",
      "Iteration: 12000. Loss: 1.0853474140167236. Rolling Loss Avg: 1.2126929047638928. Rolling Val Loss Avg: 0.9804948700798882. Elapsed Time: 39m 56s. accuracy: 16\n",
      "Iteration: 12500. Loss: 0.9203566312789917. Rolling Loss Avg: 1.2050529224983588. Rolling Val Loss Avg: 0.93625334015599. Elapsed Time: 41m 36s. accuracy: 16\n",
      "Iteration: 13000. Loss: 0.966976523399353. Rolling Loss Avg: 1.1947479001784012. Rolling Val Loss Avg: 0.8949700947161074. Elapsed Time: 43m 17s. accuracy: 15\n",
      "Iteration: 13500. Loss: 0.8930208086967468. Rolling Loss Avg: 1.18474190740283. Rolling Val Loss Avg: 0.8786445171744736. Elapsed Time: 44m 58s. accuracy: 14\n",
      "Iteration: 14000. Loss: 1.0661734342575073. Rolling Loss Avg: 1.1752150085651656. Rolling Val Loss Avg: 0.9503821554007353. Elapsed Time: 46m 38s. accuracy: 16\n",
      "Iteration: 14500. Loss: 0.7127553224563599. Rolling Loss Avg: 1.1660474365591615. Rolling Val Loss Avg: 0.9529081825856809. Elapsed Time: 48m 19s. accuracy: 15\n",
      "Iteration: 15000. Loss: 0.6999731659889221. Rolling Loss Avg: 1.1564912810841208. Rolling Val Loss Avg: 0.901242733001709. Elapsed Time: 50m 0s. accuracy: 14\n",
      "Iteration: 15500. Loss: 0.8680509924888611. Rolling Loss Avg: 1.1484779276299357. Rolling Val Loss Avg: 0.9201348026593527. Elapsed Time: 51m 41s. accuracy: 13\n",
      "Iteration: 16000. Loss: 0.8657274842262268. Rolling Loss Avg: 1.141001979266619. Rolling Val Loss Avg: 0.9060811422489308. Elapsed Time: 53m 22s. accuracy: 12\n",
      "Iteration: 16500. Loss: 0.6059553623199463. Rolling Loss Avg: 1.1331402163300817. Rolling Val Loss Avg: 0.8773784107632108. Elapsed Time: 55m 3s. accuracy: 15\n",
      "Iteration: 17000. Loss: 0.7490326762199402. Rolling Loss Avg: 1.1244988315253108. Rolling Val Loss Avg: 0.7952596810128953. Elapsed Time: 56m 44s. accuracy: 14\n",
      "Iteration: 17500. Loss: 0.8653577566146851. Rolling Loss Avg: 1.1158394182070985. Rolling Val Loss Avg: 0.8877915099815086. Elapsed Time: 58m 25s. accuracy: 15\n",
      "Iteration: 18000. Loss: 0.9270776510238647. Rolling Loss Avg: 1.1077269478149794. Rolling Val Loss Avg: 0.8470393573796308. Elapsed Time: 60m 6s. accuracy: 15\n",
      "Iteration: 18500. Loss: 0.8616549372673035. Rolling Loss Avg: 1.1001360512580054. Rolling Val Loss Avg: 0.8378708726829953. Elapsed Time: 61m 47s. accuracy: 14\n",
      "Iteration: 19000. Loss: 0.6789529323577881. Rolling Loss Avg: 1.091965512044304. Rolling Val Loss Avg: 0.8027962887728656. Elapsed Time: 63m 29s. accuracy: 13\n",
      "Iteration: 19500. Loss: 0.8669000864028931. Rolling Loss Avg: 1.0831446957404196. Rolling Val Loss Avg: 0.7760927732344027. Elapsed Time: 65m 10s. accuracy: 14\n",
      "Iteration: 20000. Loss: 0.5368894338607788. Rolling Loss Avg: 1.0738406482577163. Rolling Val Loss Avg: 0.653739333152771. Elapsed Time: 66m 51s. accuracy: 15\n",
      "Iteration: 20500. Loss: 0.7732177376747131. Rolling Loss Avg: 1.0647653993990507. Rolling Val Loss Avg: 0.706717347657239. Elapsed Time: 68m 33s. accuracy: 15\n",
      "Iteration: 21000. Loss: 0.5154800415039062. Rolling Loss Avg: 1.0547593375153952. Rolling Val Loss Avg: 0.6084835838388514. Elapsed Time: 70m 14s. accuracy: 14\n",
      "Iteration: 21500. Loss: 0.779625415802002. Rolling Loss Avg: 1.0442128020075054. Rolling Val Loss Avg: 0.5679570005999671. Elapsed Time: 71m 55s. accuracy: 16\n",
      "Iteration: 22000. Loss: 0.5949198603630066. Rolling Loss Avg: 1.033061165231032. Rolling Val Loss Avg: 0.5205200204142818. Elapsed Time: 73m 36s. accuracy: 17\n",
      "Iteration: 22500. Loss: 0.6684767603874207. Rolling Loss Avg: 1.0231677944889535. Rolling Val Loss Avg: 0.7514604285911277. Elapsed Time: 75m 17s. accuracy: 13\n",
      "Iteration: 23000. Loss: 0.38219937682151794. Rolling Loss Avg: 1.0136402917714964. Rolling Val Loss Avg: 0.5085380463688461. Elapsed Time: 76m 59s. accuracy: 18\n",
      "Iteration: 23500. Loss: 0.6152044534683228. Rolling Loss Avg: 1.0066932711887702. Rolling Val Loss Avg: 0.530346076797556. Elapsed Time: 78m 39s. accuracy: 16\n",
      "Iteration: 24000. Loss: 0.962135910987854. Rolling Loss Avg: 0.9973826551735637. Rolling Val Loss Avg: 0.5329429098853359. Elapsed Time: 80m 20s. accuracy: 14\n",
      "Iteration: 24500. Loss: 0.41720259189605713. Rolling Loss Avg: 0.98749214568971. Rolling Val Loss Avg: 0.43323425562293444. Elapsed Time: 82m 1s. accuracy: 13\n",
      "Iteration: 25000. Loss: 0.5114409327507019. Rolling Loss Avg: 0.9767941947264851. Rolling Val Loss Avg: 0.41994991567399764. Elapsed Time: 83m 41s. accuracy: 12\n",
      "Iteration: 25500. Loss: 0.46421515941619873. Rolling Loss Avg: 0.9653853894631015. Rolling Val Loss Avg: 0.40856893526183236. Elapsed Time: 85m 22s. accuracy: 14\n",
      "Iteration: 26000. Loss: 0.32730939984321594. Rolling Loss Avg: 0.9537453401883489. Rolling Val Loss Avg: 0.3217577481711352. Elapsed Time: 87m 3s. accuracy: 15\n",
      "Iteration: 26500. Loss: 0.491289347410202. Rolling Loss Avg: 0.9420590009670349. Rolling Val Loss Avg: 0.3213413561935778. Elapsed Time: 88m 44s. accuracy: 13\n",
      "Iteration: 27000. Loss: 0.36942896246910095. Rolling Loss Avg: 0.9301556337796283. Rolling Val Loss Avg: 0.2675902556490015. Elapsed Time: 90m 24s. accuracy: 14\n",
      "Iteration: 27500. Loss: 0.3780865967273712. Rolling Loss Avg: 0.9183545176590413. Rolling Val Loss Avg: 0.26773334818857686. Elapsed Time: 92m 5s. accuracy: 12\n",
      "Iteration: 28000. Loss: 0.18072232604026794. Rolling Loss Avg: 0.9065565110069271. Rolling Val Loss Avg: 0.22790628561267146. Elapsed Time: 93m 46s. accuracy: 14\n",
      "Iteration: 28500. Loss: 0.20818015933036804. Rolling Loss Avg: 0.8948776452801762. Rolling Val Loss Avg: 0.2683954542433774. Elapsed Time: 95m 27s. accuracy: 16\n",
      "Iteration: 29000. Loss: 0.3011992275714874. Rolling Loss Avg: 0.8839577430599924. Rolling Val Loss Avg: 0.36154421391310515. Elapsed Time: 97m 8s. accuracy: 15\n",
      "Iteration: 29500. Loss: 0.3186539113521576. Rolling Loss Avg: 0.8734309293095094. Rolling Val Loss Avg: 0.24161117551503358. Elapsed Time: 98m 48s. accuracy: 12\n",
      "Iteration: 30000. Loss: 0.2291288524866104. Rolling Loss Avg: 0.8627506364166114. Rolling Val Loss Avg: 0.2039319850780346. Elapsed Time: 100m 29s. accuracy: 14\n",
      "Iteration: 30500. Loss: 0.23734043538570404. Rolling Loss Avg: 0.8520600516140834. Rolling Val Loss Avg: 0.2021476587763539. Elapsed Time: 102m 11s. accuracy: 17\n",
      "Iteration: 31000. Loss: 0.13046517968177795. Rolling Loss Avg: 0.8424710595825211. Rolling Val Loss Avg: 0.19252951001679455. Elapsed Time: 103m 51s. accuracy: 18\n",
      "Iteration: 31500. Loss: 0.12465903162956238. Rolling Loss Avg: 0.8325097146572482. Rolling Val Loss Avg: 0.1871105515294605. Elapsed Time: 105m 32s. accuracy: 17\n",
      "Iteration: 32000. Loss: 0.19798871874809265. Rolling Loss Avg: 0.8227456016183432. Rolling Val Loss Avg: 0.19985450528286122. Elapsed Time: 107m 13s. accuracy: 14\n",
      "Iteration: 32500. Loss: 0.11888068914413452. Rolling Loss Avg: 0.8130322729549805. Rolling Val Loss Avg: 0.16929982878543712. Elapsed Time: 108m 54s. accuracy: 13\n",
      "Iteration: 33000. Loss: 0.14714288711547852. Rolling Loss Avg: 0.8036425256304862. Rolling Val Loss Avg: 0.17918134618688514. Elapsed Time: 110m 35s. accuracy: 15\n",
      "Iteration: 33500. Loss: 0.22691653668880463. Rolling Loss Avg: 0.7963248147583304. Rolling Val Loss Avg: 0.24585873385270438. Elapsed Time: 112m 15s. accuracy: 15\n",
      "Iteration: 34000. Loss: 0.21622079610824585. Rolling Loss Avg: 0.7882975553728048. Rolling Val Loss Avg: 0.25928185217910343. Elapsed Time: 113m 56s. accuracy: 15\n",
      "Iteration: 34500. Loss: 0.27810508012771606. Rolling Loss Avg: 0.7802528363334618. Rolling Val Loss Avg: 0.2248097117300387. Elapsed Time: 115m 36s. accuracy: 12\n",
      "Iteration: 35000. Loss: 0.22155602276325226. Rolling Loss Avg: 0.7718132442056967. Rolling Val Loss Avg: 0.19258524422292356. Elapsed Time: 117m 17s. accuracy: 16\n",
      "Iteration: 35500. Loss: 0.25020092725753784. Rolling Loss Avg: 0.7637489401480873. Rolling Val Loss Avg: 0.16372550472065253. Elapsed Time: 118m 58s. accuracy: 12\n",
      "Iteration: 36000. Loss: 0.15373259782791138. Rolling Loss Avg: 0.7556118437857009. Rolling Val Loss Avg: 0.17308073684021277. Elapsed Time: 120m 39s. accuracy: 17\n",
      "Iteration: 36500. Loss: 0.0842268168926239. Rolling Loss Avg: 0.7476286680185613. Rolling Val Loss Avg: 0.15650081910468913. Elapsed Time: 122m 20s. accuracy: 15\n",
      "Iteration: 37000. Loss: 0.1436769962310791. Rolling Loss Avg: 0.7399991193461014. Rolling Val Loss Avg: 0.18368919248934146. Elapsed Time: 124m 1s. accuracy: 14\n",
      "Iteration: 37500. Loss: 0.11189034581184387. Rolling Loss Avg: 0.7325087240555519. Rolling Val Loss Avg: 0.14489322238498265. Elapsed Time: 125m 41s. accuracy: 12\n",
      "Iteration: 38000. Loss: 0.22023418545722961. Rolling Loss Avg: 0.7251760427475646. Rolling Val Loss Avg: 0.17949855658743116. Elapsed Time: 127m 22s. accuracy: 13\n",
      "Iteration: 38500. Loss: 0.11534255743026733. Rolling Loss Avg: 0.7179648434953105. Rolling Val Loss Avg: 0.16502128927795975. Elapsed Time: 129m 3s. accuracy: 15\n",
      "Iteration: 39000. Loss: 0.16195069253444672. Rolling Loss Avg: 0.710797124944211. Rolling Val Loss Avg: 0.1369337875533987. Elapsed Time: 130m 44s. accuracy: 13\n",
      "Iteration: 39500. Loss: 0.11810991168022156. Rolling Loss Avg: 0.7038429758278848. Rolling Val Loss Avg: 0.15905110206868914. Elapsed Time: 132m 25s. accuracy: 12\n",
      "Iteration: 40000. Loss: 0.1708330661058426. Rolling Loss Avg: 0.6969599257302026. Rolling Val Loss Avg: 0.15215528618406365. Elapsed Time: 134m 6s. accuracy: 16\n",
      "Iteration: 40500. Loss: 0.0580371618270874. Rolling Loss Avg: 0.690395513702824. Rolling Val Loss Avg: 0.15952082364647477. Elapsed Time: 135m 46s. accuracy: 17\n",
      "Iteration: 41000. Loss: 0.10508304834365845. Rolling Loss Avg: 0.6838253745058479. Rolling Val Loss Avg: 0.14056232240464953. Elapsed Time: 137m 27s. accuracy: 14\n",
      "Iteration: 41500. Loss: 0.16239133477210999. Rolling Loss Avg: 0.6774378529568958. Rolling Val Loss Avg: 0.1881221157533151. Elapsed Time: 139m 9s. accuracy: 14\n",
      "Iteration: 42000. Loss: 0.11982893943786621. Rolling Loss Avg: 0.6713029621182497. Rolling Val Loss Avg: 0.1192384715433474. Elapsed Time: 140m 50s. accuracy: 12\n",
      "Iteration: 42500. Loss: 0.2830040752887726. Rolling Loss Avg: 0.6651696570687248. Rolling Val Loss Avg: 0.14827454366065837. Elapsed Time: 142m 31s. accuracy: 17\n",
      "Iteration: 43000. Loss: 0.08912265300750732. Rolling Loss Avg: 0.6591582957539364. Rolling Val Loss Avg: 0.15158738802980493. Elapsed Time: 144m 13s. accuracy: 10\n",
      "Iteration: 43500. Loss: 0.14787834882736206. Rolling Loss Avg: 0.6532604825414873. Rolling Val Loss Avg: 0.13296119151292024. Elapsed Time: 145m 54s. accuracy: 13\n",
      "Iteration: 44000. Loss: 0.17531949281692505. Rolling Loss Avg: 0.6474957126776187. Rolling Val Loss Avg: 0.11894560356934865. Elapsed Time: 147m 35s. accuracy: 12\n",
      "Iteration: 44500. Loss: 0.056522250175476074. Rolling Loss Avg: 0.6418816729432768. Rolling Val Loss Avg: 0.16428193339595087. Elapsed Time: 149m 16s. accuracy: 15\n",
      "Iteration: 45000. Loss: 0.06433531641960144. Rolling Loss Avg: 0.6363604339729354. Rolling Val Loss Avg: 0.17464415618666895. Elapsed Time: 150m 57s. accuracy: 13\n",
      "Iteration: 45500. Loss: 0.223667711019516. Rolling Loss Avg: 0.6310342524225813. Rolling Val Loss Avg: 0.13647527827156913. Elapsed Time: 152m 38s. accuracy: 15\n",
      "Iteration: 46000. Loss: 0.04881444573402405. Rolling Loss Avg: 0.6256680646133626. Rolling Val Loss Avg: 0.15173682736025917. Elapsed Time: 154m 19s. accuracy: 12\n",
      "Iteration: 46500. Loss: 0.10500264167785645. Rolling Loss Avg: 0.6206994811956381. Rolling Val Loss Avg: 0.15209920759554263. Elapsed Time: 156m 0s. accuracy: 15\n",
      "Iteration: 47000. Loss: 0.16193795204162598. Rolling Loss Avg: 0.6156725878937545. Rolling Val Loss Avg: 0.13220374672501176. Elapsed Time: 157m 39s. accuracy: 14\n",
      "Iteration: 47500. Loss: 0.15403161942958832. Rolling Loss Avg: 0.6108172953518433. Rolling Val Loss Avg: 0.14114801088968912. Elapsed Time: 159m 20s. accuracy: 14\n",
      "Iteration: 48000. Loss: 0.07188311219215393. Rolling Loss Avg: 0.6060853011552096. Rolling Val Loss Avg: 0.15431029266781277. Elapsed Time: 161m 0s. accuracy: 18\n",
      "Iteration: 48500. Loss: 0.08143538236618042. Rolling Loss Avg: 0.6012935337955855. Rolling Val Loss Avg: 0.1402295794751909. Elapsed Time: 162m 40s. accuracy: 17\n",
      "Iteration: 49000. Loss: 0.43294867873191833. Rolling Loss Avg: 0.5965993597728569. Rolling Val Loss Avg: 0.14320287881074129. Elapsed Time: 164m 21s. accuracy: 12\n",
      "Iteration: 49500. Loss: 0.12673693895339966. Rolling Loss Avg: 0.5919855323552541. Rolling Val Loss Avg: 0.13255170815520817. Elapsed Time: 166m 1s. accuracy: 12\n",
      "Iteration: 50000. Loss: 0.12959304451942444. Rolling Loss Avg: 0.5877192069642752. Rolling Val Loss Avg: 0.12330159544944763. Elapsed Time: 167m 41s. accuracy: 15\n",
      "Iteration: 50500. Loss: 0.2612273395061493. Rolling Loss Avg: 0.5833115389468833. Rolling Val Loss Avg: 0.17646345220230245. Elapsed Time: 169m 22s. accuracy: 14\n",
      "Iteration: 51000. Loss: 0.06399965286254883. Rolling Loss Avg: 0.5790277200623126. Rolling Val Loss Avg: 0.1578983857675835. Elapsed Time: 171m 2s. accuracy: 17\n",
      "Iteration: 51500. Loss: 0.22038531303405762. Rolling Loss Avg: 0.574793226891498. Rolling Val Loss Avg: 0.13133670941547113. Elapsed Time: 172m 43s. accuracy: 14\n",
      "Iteration: 52000. Loss: 0.06733274459838867. Rolling Loss Avg: 0.5706597345312298. Rolling Val Loss Avg: 0.13414591550827026. Elapsed Time: 174m 24s. accuracy: 14\n",
      "Iteration: 52500. Loss: 0.20328190922737122. Rolling Loss Avg: 0.5665955760060455. Rolling Val Loss Avg: 0.11601856902793602. Elapsed Time: 176m 5s. accuracy: 13\n",
      "Iteration: 53000. Loss: 0.1515774130821228. Rolling Loss Avg: 0.5626482661625198. Rolling Val Loss Avg: 0.17123968733681572. Elapsed Time: 177m 45s. accuracy: 17\n",
      "Iteration: 53500. Loss: 0.33853092789649963. Rolling Loss Avg: 0.5586802561474378. Rolling Val Loss Avg: 0.13181856771310171. Elapsed Time: 179m 25s. accuracy: 15\n",
      "Iteration: 54000. Loss: 0.07846930623054504. Rolling Loss Avg: 0.5548136575731714. Rolling Val Loss Avg: 0.1271447793201164. Elapsed Time: 181m 6s. accuracy: 15\n",
      "Iteration: 54500. Loss: 0.04968765377998352. Rolling Loss Avg: 0.5510452202493599. Rolling Val Loss Avg: 0.125412220204318. Elapsed Time: 182m 46s. accuracy: 13\n",
      "Iteration: 55000. Loss: 0.1512148082256317. Rolling Loss Avg: 0.5472962595307695. Rolling Val Loss Avg: 0.1123551235154823. Elapsed Time: 184m 27s. accuracy: 17\n",
      "Iteration: 55500. Loss: 0.04427477717399597. Rolling Loss Avg: 0.5436338346184177. Rolling Val Loss Avg: 0.09756072583021941. Elapsed Time: 186m 7s. accuracy: 12\n",
      "Iteration: 56000. Loss: 0.16508454084396362. Rolling Loss Avg: 0.5399461882318467. Rolling Val Loss Avg: 0.12882427043384975. Elapsed Time: 187m 48s. accuracy: 14\n",
      "Iteration: 56500. Loss: 0.3354661166667938. Rolling Loss Avg: 0.5363427086203789. Rolling Val Loss Avg: 0.14468146363894144. Elapsed Time: 189m 29s. accuracy: 12\n",
      "Iteration: 57000. Loss: 0.13962501287460327. Rolling Loss Avg: 0.5328138730984123. Rolling Val Loss Avg: 0.15325068323700516. Elapsed Time: 191m 9s. accuracy: 9\n",
      "Iteration: 57500. Loss: 0.2272064983844757. Rolling Loss Avg: 0.5294510966177511. Rolling Val Loss Avg: 0.14376476407051086. Elapsed Time: 192m 50s. accuracy: 14\n",
      "Iteration: 58000. Loss: 0.1993672251701355. Rolling Loss Avg: 0.5261296454743002. Rolling Val Loss Avg: 0.13226469357808432. Elapsed Time: 194m 31s. accuracy: 14\n",
      "Iteration: 58500. Loss: 0.09867143630981445. Rolling Loss Avg: 0.5228263769839491. Rolling Val Loss Avg: 0.12419105679900558. Elapsed Time: 196m 11s. accuracy: 15\n",
      "Iteration: 59000. Loss: 0.06039515137672424. Rolling Loss Avg: 0.5196349368170398. Rolling Val Loss Avg: 0.14553160082410882. Elapsed Time: 197m 52s. accuracy: 15\n",
      "Iteration: 59500. Loss: 0.1912883222103119. Rolling Loss Avg: 0.5163165312595034. Rolling Val Loss Avg: 0.10051926694534442. Elapsed Time: 199m 33s. accuracy: 13\n",
      "Iteration: 60000. Loss: 0.38275444507598877. Rolling Loss Avg: 0.513122340189332. Rolling Val Loss Avg: 0.12340968736895809. Elapsed Time: 201m 14s. accuracy: 16\n",
      "Iteration: 60500. Loss: 0.1978963017463684. Rolling Loss Avg: 0.5099171961712765. Rolling Val Loss Avg: 0.1387368948371322. Elapsed Time: 202m 55s. accuracy: 15\n",
      "Iteration: 61000. Loss: 0.1403380036354065. Rolling Loss Avg: 0.5067854900240343. Rolling Val Loss Avg: 0.13656473214979525. Elapsed Time: 204m 36s. accuracy: 11\n",
      "Iteration: 61500. Loss: 0.14769089221954346. Rolling Loss Avg: 0.5037991273287789. Rolling Val Loss Avg: 0.10845600565274556. Elapsed Time: 206m 17s. accuracy: 16\n",
      "Iteration: 62000. Loss: 0.0966082215309143. Rolling Loss Avg: 0.5007815686513136. Rolling Val Loss Avg: 0.1289923389752706. Elapsed Time: 207m 59s. accuracy: 15\n",
      "Iteration: 62500. Loss: 0.14326637983322144. Rolling Loss Avg: 0.49775857958833636. Rolling Val Loss Avg: 0.16688624134770147. Elapsed Time: 209m 40s. accuracy: 12\n",
      "Iteration: 63000. Loss: 0.03764545917510986. Rolling Loss Avg: 0.49488217727101264. Rolling Val Loss Avg: 0.1374059761012042. Elapsed Time: 211m 21s. accuracy: 13\n",
      "Iteration: 63500. Loss: 0.18797367811203003. Rolling Loss Avg: 0.49204596904436004. Rolling Val Loss Avg: 0.1454044470080623. Elapsed Time: 213m 2s. accuracy: 15\n",
      "Iteration: 64000. Loss: 0.15719720721244812. Rolling Loss Avg: 0.4892308698862531. Rolling Val Loss Avg: 0.12081068423059252. Elapsed Time: 214m 43s. accuracy: 13\n",
      "Iteration: 64500. Loss: 0.21561160683631897. Rolling Loss Avg: 0.4868701550424514. Rolling Val Loss Avg: 0.12905711045971624. Elapsed Time: 216m 24s. accuracy: 12\n",
      "Iteration: 65000. Loss: 0.2364751398563385. Rolling Loss Avg: 0.4842881169396234. Rolling Val Loss Avg: 0.13519262108537886. Elapsed Time: 218m 5s. accuracy: 11\n",
      "Iteration: 65500. Loss: 0.16555482149124146. Rolling Loss Avg: 0.4816068341874128. Rolling Val Loss Avg: 0.12304941040498239. Elapsed Time: 219m 45s. accuracy: 12\n",
      "Iteration: 66000. Loss: 0.26897940039634705. Rolling Loss Avg: 0.47895698170636025. Rolling Val Loss Avg: 0.15543071373745246. Elapsed Time: 221m 26s. accuracy: 11\n",
      "Iteration: 66500. Loss: 0.13392019271850586. Rolling Loss Avg: 0.47632531671261896. Rolling Val Loss Avg: 0.12160884616551576. Elapsed Time: 223m 7s. accuracy: 16\n",
      "Iteration: 67000. Loss: 0.14076969027519226. Rolling Loss Avg: 0.47374009209395085. Rolling Val Loss Avg: 0.1131469640466902. Elapsed Time: 224m 47s. accuracy: 13\n",
      "Iteration: 67500. Loss: 0.15184107422828674. Rolling Loss Avg: 0.47143762086382995. Rolling Val Loss Avg: 0.1576287117269304. Elapsed Time: 226m 28s. accuracy: 16\n",
      "Iteration: 68000. Loss: 0.0520993173122406. Rolling Loss Avg: 0.4689995825647426. Rolling Val Loss Avg: 0.12228995837547162. Elapsed Time: 228m 9s. accuracy: 14\n",
      "Iteration: 68500. Loss: 0.17179268598556519. Rolling Loss Avg: 0.46655091046795366. Rolling Val Loss Avg: 0.1022837581457915. Elapsed Time: 229m 50s. accuracy: 13\n",
      "Iteration: 69000. Loss: 0.21201732754707336. Rolling Loss Avg: 0.46408382150011185. Rolling Val Loss Avg: 0.1318518508363653. Elapsed Time: 231m 31s. accuracy: 13\n",
      "Iteration: 69500. Loss: 0.08485421538352966. Rolling Loss Avg: 0.4616103293531632. Rolling Val Loss Avg: 0.12349919478098552. Elapsed Time: 233m 11s. accuracy: 18\n",
      "Iteration: 70000. Loss: 0.13777250051498413. Rolling Loss Avg: 0.45924692783214743. Rolling Val Loss Avg: 0.15434119436475965. Elapsed Time: 234m 52s. accuracy: 14\n",
      "Iteration: 70500. Loss: 0.08282122015953064. Rolling Loss Avg: 0.4568983541628758. Rolling Val Loss Avg: 0.12242962144039295. Elapsed Time: 236m 33s. accuracy: 15\n",
      "Iteration: 71000. Loss: 0.1036769449710846. Rolling Loss Avg: 0.45482162353155525. Rolling Val Loss Avg: 0.13761839160212763. Elapsed Time: 238m 14s. accuracy: 15\n",
      "Iteration: 71500. Loss: 0.038349419832229614. Rolling Loss Avg: 0.45256821845419276. Rolling Val Loss Avg: 0.1273171416035405. Elapsed Time: 239m 54s. accuracy: 18\n",
      "Iteration: 72000. Loss: 0.20834437012672424. Rolling Loss Avg: 0.45029338813247277. Rolling Val Loss Avg: 0.12402534484863281. Elapsed Time: 241m 35s. accuracy: 13\n",
      "Iteration: 72500. Loss: 0.10241630673408508. Rolling Loss Avg: 0.4480903169395309. Rolling Val Loss Avg: 0.15812506112787458. Elapsed Time: 243m 15s. accuracy: 14\n",
      "Iteration: 73000. Loss: 0.02123183012008667. Rolling Loss Avg: 0.44619178054991676. Rolling Val Loss Avg: 0.1336140020026101. Elapsed Time: 244m 56s. accuracy: 15\n",
      "Iteration: 73500. Loss: 0.4036587178707123. Rolling Loss Avg: 0.4440826436050623. Rolling Val Loss Avg: 0.14167600428616559. Elapsed Time: 246m 36s. accuracy: 16\n",
      "Iteration: 74000. Loss: 0.051131993532180786. Rolling Loss Avg: 0.44202011780078937. Rolling Val Loss Avg: 0.12692699222653. Elapsed Time: 248m 16s. accuracy: 15\n",
      "Iteration: 74500. Loss: 0.07427582144737244. Rolling Loss Avg: 0.43995390874966905. Rolling Val Loss Avg: 0.14827529699714095. Elapsed Time: 249m 57s. accuracy: 15\n",
      "Iteration: 75000. Loss: 0.19903740286827087. Rolling Loss Avg: 0.4379604520564448. Rolling Val Loss Avg: 0.1479323903719584. Elapsed Time: 251m 37s. accuracy: 14\n",
      "Iteration: 75500. Loss: 0.11501824855804443. Rolling Loss Avg: 0.4359543628526747. Rolling Val Loss Avg: 0.10641005414503592. Elapsed Time: 253m 18s. accuracy: 12\n",
      "Iteration: 76000. Loss: 0.057622939348220825. Rolling Loss Avg: 0.43392567236742435. Rolling Val Loss Avg: 0.13809927856480633. Elapsed Time: 254m 58s. accuracy: 14\n",
      "Iteration: 76500. Loss: 0.26601868867874146. Rolling Loss Avg: 0.43197429109224456. Rolling Val Loss Avg: 0.16061988914454425. Elapsed Time: 256m 38s. accuracy: 14\n",
      "Iteration: 77000. Loss: 0.10183769464492798. Rolling Loss Avg: 0.4300453479679431. Rolling Val Loss Avg: 0.1541069679790073. Elapsed Time: 258m 19s. accuracy: 16\n",
      "Iteration: 77500. Loss: 0.053626060485839844. Rolling Loss Avg: 0.4280778228176423. Rolling Val Loss Avg: 0.15885439956629718. Elapsed Time: 260m 0s. accuracy: 17\n",
      "Iteration: 78000. Loss: 0.18973559141159058. Rolling Loss Avg: 0.4261691838356167. Rolling Val Loss Avg: 0.15607931161368335. Elapsed Time: 261m 40s. accuracy: 14\n",
      "Iteration: 78500. Loss: 0.21890243887901306. Rolling Loss Avg: 0.4243093973161615. Rolling Val Loss Avg: 0.12461201681031121. Elapsed Time: 263m 21s. accuracy: 14\n",
      "Iteration: 79000. Loss: 0.06839674711227417. Rolling Loss Avg: 0.42246415546337374. Rolling Val Loss Avg: 0.10095139841238658. Elapsed Time: 265m 2s. accuracy: 14\n",
      "Iteration: 79500. Loss: 0.1815156638622284. Rolling Loss Avg: 0.42064193820079476. Rolling Val Loss Avg: 0.11473906757654967. Elapsed Time: 266m 42s. accuracy: 12\n",
      "Iteration: 80000. Loss: 0.2477342039346695. Rolling Loss Avg: 0.41883110330299467. Rolling Val Loss Avg: 0.13205319974157545. Elapsed Time: 268m 23s. accuracy: 12\n",
      "Iteration: 80500. Loss: 0.23924490809440613. Rolling Loss Avg: 0.4170305695321296. Rolling Val Loss Avg: 0.13378265665637123. Elapsed Time: 270m 4s. accuracy: 14\n",
      "Iteration: 81000. Loss: 0.026681870222091675. Rolling Loss Avg: 0.4152545381362561. Rolling Val Loss Avg: 0.12201330341674664. Elapsed Time: 271m 45s. accuracy: 15\n",
      "Iteration: 81500. Loss: 0.15999794006347656. Rolling Loss Avg: 0.4135289820382903. Rolling Val Loss Avg: 0.13864806515199166. Elapsed Time: 273m 25s. accuracy: 17\n",
      "Iteration: 82000. Loss: 0.05232149362564087. Rolling Loss Avg: 0.4118374803099952. Rolling Val Loss Avg: 0.12847842938370174. Elapsed Time: 275m 6s. accuracy: 16\n",
      "Iteration: 82500. Loss: 0.06104287505149841. Rolling Loss Avg: 0.4100959657088573. Rolling Val Loss Avg: 0.12179321933675695. Elapsed Time: 276m 46s. accuracy: 14\n",
      "Iteration: 83000. Loss: 0.15141427516937256. Rolling Loss Avg: 0.4084143101538169. Rolling Val Loss Avg: 0.16520341568522984. Elapsed Time: 278m 26s. accuracy: 14\n",
      "Iteration: 83500. Loss: 0.17554113268852234. Rolling Loss Avg: 0.40687799672592423. Rolling Val Loss Avg: 0.1448764376066349. Elapsed Time: 280m 7s. accuracy: 13\n",
      "Iteration: 84000. Loss: 0.18121027946472168. Rolling Loss Avg: 0.4052161844089255. Rolling Val Loss Avg: 0.1320826444360945. Elapsed Time: 281m 46s. accuracy: 15\n",
      "Iteration: 84500. Loss: 0.19586244225502014. Rolling Loss Avg: 0.40354453878315044. Rolling Val Loss Avg: 0.10243438294640293. Elapsed Time: 283m 25s. accuracy: 14\n",
      "Iteration: 85000. Loss: 0.023284584283828735. Rolling Loss Avg: 0.40194755536443555. Rolling Val Loss Avg: 0.11719361775451237. Elapsed Time: 285m 5s. accuracy: 16\n",
      "Iteration: 85500. Loss: 0.2058086097240448. Rolling Loss Avg: 0.40035452881897643. Rolling Val Loss Avg: 0.10972847596362785. Elapsed Time: 286m 45s. accuracy: 16\n",
      "Iteration: 86000. Loss: 0.13476012647151947. Rolling Loss Avg: 0.39879207397355304. Rolling Val Loss Avg: 0.13684030373891196. Elapsed Time: 288m 24s. accuracy: 15\n",
      "Iteration: 86500. Loss: 0.24123185873031616. Rolling Loss Avg: 0.39727288496149626. Rolling Val Loss Avg: 0.1154349540118818. Elapsed Time: 290m 4s. accuracy: 11\n",
      "Iteration: 87000. Loss: 0.08697357773780823. Rolling Loss Avg: 0.39570592585134745. Rolling Val Loss Avg: 0.12684532944802884. Elapsed Time: 291m 42s. accuracy: 15\n",
      "Iteration: 87500. Loss: 0.21303626894950867. Rolling Loss Avg: 0.39420406651418755. Rolling Val Loss Avg: 0.13799568127702783. Elapsed Time: 293m 22s. accuracy: 13\n",
      "Iteration: 88000. Loss: 0.1510569453239441. Rolling Loss Avg: 0.39267413245167737. Rolling Val Loss Avg: 0.09907822476492988. Elapsed Time: 295m 1s. accuracy: 13\n",
      "Iteration: 88500. Loss: 0.0368940532207489. Rolling Loss Avg: 0.3911863854140317. Rolling Val Loss Avg: 0.11993726425700718. Elapsed Time: 296m 39s. accuracy: 15\n",
      "Iteration: 89000. Loss: 0.051472485065460205. Rolling Loss Avg: 0.3896676625054927. Rolling Val Loss Avg: 0.14414171488196761. Elapsed Time: 298m 17s. accuracy: 14\n",
      "Iteration: 89500. Loss: 0.10641506314277649. Rolling Loss Avg: 0.3881988792794386. Rolling Val Loss Avg: 0.12385240307560673. Elapsed Time: 299m 56s. accuracy: 15\n",
      "Iteration: 90000. Loss: 0.048971742391586304. Rolling Loss Avg: 0.38676973565807116. Rolling Val Loss Avg: 0.09375045476136384. Elapsed Time: 301m 33s. accuracy: 15\n",
      "Iteration: 90500. Loss: 0.16557180881500244. Rolling Loss Avg: 0.38530676481221954. Rolling Val Loss Avg: 0.109003237552113. Elapsed Time: 303m 12s. accuracy: 12\n",
      "Iteration: 91000. Loss: 0.09168079495429993. Rolling Loss Avg: 0.3838516495710632. Rolling Val Loss Avg: 0.16170473783104508. Elapsed Time: 304m 51s. accuracy: 13\n",
      "Iteration: 91500. Loss: 0.13821810483932495. Rolling Loss Avg: 0.38242486664060593. Rolling Val Loss Avg: 0.127629436828472. Elapsed Time: 306m 29s. accuracy: 15\n",
      "Iteration: 92000. Loss: 0.20498314499855042. Rolling Loss Avg: 0.3812091832411623. Rolling Val Loss Avg: 0.11809597743882073. Elapsed Time: 308m 7s. accuracy: 14\n",
      "Iteration: 92500. Loss: 0.0454230010509491. Rolling Loss Avg: 0.3798594798458312. Rolling Val Loss Avg: 0.12564827281015892. Elapsed Time: 309m 45s. accuracy: 14\n",
      "Iteration: 93000. Loss: 0.1406339704990387. Rolling Loss Avg: 0.3784983423838078. Rolling Val Loss Avg: 0.14940622393731717. Elapsed Time: 311m 24s. accuracy: 12\n",
      "Iteration: 93500. Loss: 0.06416717171669006. Rolling Loss Avg: 0.37712967897890454. Rolling Val Loss Avg: 0.10660448449629324. Elapsed Time: 313m 3s. accuracy: 13\n",
      "Iteration: 94000. Loss: 0.26723024249076843. Rolling Loss Avg: 0.37577365859544826. Rolling Val Loss Avg: 0.11322315092440005. Elapsed Time: 314m 41s. accuracy: 14\n",
      "Iteration: 94500. Loss: 0.12284654378890991. Rolling Loss Avg: 0.37445038721657453. Rolling Val Loss Avg: 0.1281102901255643. Elapsed Time: 316m 20s. accuracy: 16\n",
      "Iteration: 95000. Loss: 0.07657799124717712. Rolling Loss Avg: 0.373117766370673. Rolling Val Loss Avg: 0.14943597934864186. Elapsed Time: 317m 59s. accuracy: 12\n",
      "Iteration: 95500. Loss: 0.026491940021514893. Rolling Loss Avg: 0.37180189223593424. Rolling Val Loss Avg: 0.11445461599915116. Elapsed Time: 319m 37s. accuracy: 15\n",
      "Iteration: 96000. Loss: 0.12486857175827026. Rolling Loss Avg: 0.3705703760929605. Rolling Val Loss Avg: 0.15218310610011773. Elapsed Time: 321m 16s. accuracy: 13\n",
      "Iteration: 96500. Loss: 0.029170721769332886. Rolling Loss Avg: 0.36928688572858415. Rolling Val Loss Avg: 0.10505407496734902. Elapsed Time: 322m 54s. accuracy: 16\n",
      "Iteration: 97000. Loss: 0.0768236517906189. Rolling Loss Avg: 0.36801696343791407. Rolling Val Loss Avg: 0.12518339300597156. Elapsed Time: 324m 32s. accuracy: 12\n",
      "Iteration: 97500. Loss: 0.05578428506851196. Rolling Loss Avg: 0.366781948431349. Rolling Val Loss Avg: 0.13609905044237772. Elapsed Time: 326m 11s. accuracy: 15\n",
      "Iteration: 98000. Loss: 0.07815754413604736. Rolling Loss Avg: 0.3655454858142665. Rolling Val Loss Avg: 0.10840639251249808. Elapsed Time: 327m 50s. accuracy: 14\n",
      "Iteration: 98500. Loss: 0.08178117871284485. Rolling Loss Avg: 0.3643374913381507. Rolling Val Loss Avg: 0.10508649106378909. Elapsed Time: 329m 29s. accuracy: 16\n",
      "Iteration: 99000. Loss: 0.07249805331230164. Rolling Loss Avg: 0.36310868989959394. Rolling Val Loss Avg: 0.1250096885142503. Elapsed Time: 331m 8s. accuracy: 15\n",
      "Iteration: 99500. Loss: 0.18796563148498535. Rolling Loss Avg: 0.36198098351329516. Rolling Val Loss Avg: 0.15453679031795925. Elapsed Time: 332m 46s. accuracy: 13\n",
      "Iteration: 100000. Loss: 0.10997089743614197. Rolling Loss Avg: 0.36086127828293335. Rolling Val Loss Avg: 0.31327898689994105. Elapsed Time: 334m 25s. accuracy: 12\n",
      "Iteration: 100500. Loss: 0.16876837611198425. Rolling Loss Avg: 0.35968944545971254. Rolling Val Loss Avg: 0.12073498026088432. Elapsed Time: 336m 4s. accuracy: 14\n",
      "Iteration: 101000. Loss: 0.09949979186058044. Rolling Loss Avg: 0.35849444621004156. Rolling Val Loss Avg: 0.11603862102384921. Elapsed Time: 337m 42s. accuracy: 13\n",
      "Iteration: 101500. Loss: 0.02893182635307312. Rolling Loss Avg: 0.35732727097404543. Rolling Val Loss Avg: 0.14040738123434562. Elapsed Time: 339m 20s. accuracy: 18\n",
      "Iteration: 102000. Loss: 0.1753171980381012. Rolling Loss Avg: 0.35618448901754496. Rolling Val Loss Avg: 0.13364183130087676. Elapsed Time: 340m 59s. accuracy: 12\n",
      "Iteration: 102500. Loss: 0.031212300062179565. Rolling Loss Avg: 0.3550650588322348. Rolling Val Loss Avg: 0.11194386416011387. Elapsed Time: 342m 37s. accuracy: 12\n",
      "Iteration: 103000. Loss: 0.07946665585041046. Rolling Loss Avg: 0.3539342233241099. Rolling Val Loss Avg: 0.1302340003075423. Elapsed Time: 344m 15s. accuracy: 10\n",
      "Iteration: 103500. Loss: 0.1311635971069336. Rolling Loss Avg: 0.35281336353855564. Rolling Val Loss Avg: 0.11952435363222051. Elapsed Time: 345m 53s. accuracy: 15\n",
      "Iteration: 104000. Loss: 0.14729198813438416. Rolling Loss Avg: 0.35178524630455854. Rolling Val Loss Avg: 0.12948770931473486. Elapsed Time: 347m 31s. accuracy: 17\n",
      "Iteration: 104500. Loss: 0.12167823314666748. Rolling Loss Avg: 0.35071705587711555. Rolling Val Loss Avg: 0.15116546385818058. Elapsed Time: 349m 9s. accuracy: 14\n",
      "Iteration: 105000. Loss: 0.04805225133895874. Rolling Loss Avg: 0.3497096851108049. Rolling Val Loss Avg: 0.1381716248061922. Elapsed Time: 350m 46s. accuracy: 14\n",
      "Iteration: 105500. Loss: 0.14176252484321594. Rolling Loss Avg: 0.3486130628668267. Rolling Val Loss Avg: 0.10288337756086278. Elapsed Time: 352m 24s. accuracy: 10\n",
      "Iteration: 106000. Loss: 0.04853615164756775. Rolling Loss Avg: 0.34757632861531473. Rolling Val Loss Avg: 0.1272011360636464. Elapsed Time: 354m 2s. accuracy: 12\n",
      "Iteration: 106500. Loss: 0.033087968826293945. Rolling Loss Avg: 0.3465314999811381. Rolling Val Loss Avg: 0.10229600855597744. Elapsed Time: 355m 42s. accuracy: 16\n",
      "Iteration: 107000. Loss: 0.19252675771713257. Rolling Loss Avg: 0.3454946294024312. Rolling Val Loss Avg: 0.12457521259784698. Elapsed Time: 357m 21s. accuracy: 15\n",
      "Iteration: 107500. Loss: 0.16401132941246033. Rolling Loss Avg: 0.34446709090566136. Rolling Val Loss Avg: 0.1151657501856486. Elapsed Time: 359m 0s. accuracy: 11\n",
      "Iteration: 108000. Loss: 0.09189960360527039. Rolling Loss Avg: 0.3434612573769067. Rolling Val Loss Avg: 0.1340635159501323. Elapsed Time: 360m 38s. accuracy: 16\n",
      "Iteration: 108500. Loss: 0.10392135381698608. Rolling Loss Avg: 0.3424680925827903. Rolling Val Loss Avg: 0.13909433561342735. Elapsed Time: 362m 17s. accuracy: 15\n",
      "Iteration: 109000. Loss: 0.04763725399971008. Rolling Loss Avg: 0.34147906296119296. Rolling Val Loss Avg: 0.10448096085477758. Elapsed Time: 363m 56s. accuracy: 15\n",
      "Iteration: 109500. Loss: 0.25806525349617004. Rolling Loss Avg: 0.34054151107541597. Rolling Val Loss Avg: 0.10385530745541607. Elapsed Time: 365m 34s. accuracy: 14\n",
      "Iteration: 110000. Loss: 0.04228699207305908. Rolling Loss Avg: 0.33958272704589826. Rolling Val Loss Avg: 0.0972453135031241. Elapsed Time: 367m 13s. accuracy: 13\n",
      "Iteration: 110500. Loss: 0.22528988122940063. Rolling Loss Avg: 0.3385887124251383. Rolling Val Loss Avg: 0.1295334349075953. Elapsed Time: 368m 52s. accuracy: 13\n",
      "Iteration: 111000. Loss: 0.1682654321193695. Rolling Loss Avg: 0.3376011403180173. Rolling Val Loss Avg: 0.12057472268740337. Elapsed Time: 370m 30s. accuracy: 15\n",
      "Iteration: 111500. Loss: 0.5036277770996094. Rolling Loss Avg: 0.3366544966954509. Rolling Val Loss Avg: 0.1237695073639905. Elapsed Time: 372m 9s. accuracy: 16\n",
      "Iteration: 112000. Loss: 0.2017725110054016. Rolling Loss Avg: 0.33569794889459686. Rolling Val Loss Avg: 0.14095235091668587. Elapsed Time: 373m 48s. accuracy: 14\n",
      "Iteration: 112500. Loss: 0.07176914811134338. Rolling Loss Avg: 0.3347708600634179. Rolling Val Loss Avg: 0.13317953712410396. Elapsed Time: 375m 26s. accuracy: 11\n",
      "Iteration: 113000. Loss: 0.021606624126434326. Rolling Loss Avg: 0.3338216515958418. Rolling Val Loss Avg: 0.13375384057009662. Elapsed Time: 377m 5s. accuracy: 13\n",
      "Iteration: 113500. Loss: 0.18159714341163635. Rolling Loss Avg: 0.33292390432704155. Rolling Val Loss Avg: 0.11421279443634881. Elapsed Time: 378m 44s. accuracy: 15\n",
      "Iteration: 114000. Loss: 0.08625012636184692. Rolling Loss Avg: 0.33200321778880004. Rolling Val Loss Avg: 0.10652794661345305. Elapsed Time: 380m 23s. accuracy: 14\n",
      "Iteration: 114500. Loss: 0.03241521120071411. Rolling Loss Avg: 0.3310774631465825. Rolling Val Loss Avg: 0.1254674698467608. Elapsed Time: 382m 2s. accuracy: 17\n",
      "Iteration: 115000. Loss: 0.2646508812904358. Rolling Loss Avg: 0.3301968914215617. Rolling Val Loss Avg: 0.14995702162936883. Elapsed Time: 383m 41s. accuracy: 14\n",
      "Iteration: 115500. Loss: 0.12956026196479797. Rolling Loss Avg: 0.3293179407379202. Rolling Val Loss Avg: 0.14137206143803066. Elapsed Time: 385m 20s. accuracy: 13\n",
      "Iteration: 116000. Loss: 0.09692105650901794. Rolling Loss Avg: 0.3284088711536559. Rolling Val Loss Avg: 0.13733583799114935. Elapsed Time: 387m 0s. accuracy: 11\n",
      "Iteration: 116500. Loss: 0.11753594875335693. Rolling Loss Avg: 0.3275347213920256. Rolling Val Loss Avg: 0.11922209516719535. Elapsed Time: 388m 38s. accuracy: 12\n",
      "Iteration: 117000. Loss: 0.03146412968635559. Rolling Loss Avg: 0.3266731446730991. Rolling Val Loss Avg: 0.09814094338152143. Elapsed Time: 390m 17s. accuracy: 12\n",
      "Iteration: 117500. Loss: 0.3630406856536865. Rolling Loss Avg: 0.3258031956445223. Rolling Val Loss Avg: 0.10277200462641539. Elapsed Time: 391m 56s. accuracy: 14\n",
      "Iteration: 118000. Loss: 0.35370326042175293. Rolling Loss Avg: 0.3249325503985592. Rolling Val Loss Avg: 0.1634184999598397. Elapsed Time: 393m 35s. accuracy: 13\n",
      "Iteration: 118500. Loss: 0.17650392651557922. Rolling Loss Avg: 0.3240634807241296. Rolling Val Loss Avg: 0.12376662095387776. Elapsed Time: 395m 14s. accuracy: 13\n",
      "Iteration: 119000. Loss: 0.20151877403259277. Rolling Loss Avg: 0.3232362795951077. Rolling Val Loss Avg: 0.13253336592956824. Elapsed Time: 396m 53s. accuracy: 13\n",
      "Iteration: 119500. Loss: 0.11317053437232971. Rolling Loss Avg: 0.32240011177049843. Rolling Val Loss Avg: 0.1376625100771586. Elapsed Time: 398m 31s. accuracy: 14\n",
      "Iteration: 120000. Loss: 0.10981905460357666. Rolling Loss Avg: 0.3215578475404706. Rolling Val Loss Avg: 0.12677890834984956. Elapsed Time: 400m 9s. accuracy: 14\n",
      "Iteration: 120500. Loss: 0.10221165418624878. Rolling Loss Avg: 0.3207579605539988. Rolling Val Loss Avg: 0.12333078296096237. Elapsed Time: 401m 48s. accuracy: 15\n",
      "Iteration: 121000. Loss: 0.13505706191062927. Rolling Loss Avg: 0.3199357600478828. Rolling Val Loss Avg: 0.14727454328978504. Elapsed Time: 403m 27s. accuracy: 13\n",
      "Iteration: 121500. Loss: 0.02497732639312744. Rolling Loss Avg: 0.3191463367258193. Rolling Val Loss Avg: 0.12837890728756232. Elapsed Time: 405m 5s. accuracy: 13\n",
      "Iteration: 122000. Loss: 0.025939971208572388. Rolling Loss Avg: 0.3183249052427375. Rolling Val Loss Avg: 0.11478093542434552. Elapsed Time: 406m 43s. accuracy: 12\n",
      "Iteration: 122500. Loss: 0.03472113609313965. Rolling Loss Avg: 0.3175889377010321. Rolling Val Loss Avg: 0.16946509370097407. Elapsed Time: 408m 22s. accuracy: 14\n",
      "Iteration: 123000. Loss: 0.17519676685333252. Rolling Loss Avg: 0.31679673749945364. Rolling Val Loss Avg: 0.11093543856232255. Elapsed Time: 410m 0s. accuracy: 14\n",
      "Iteration: 123500. Loss: 0.14583227038383484. Rolling Loss Avg: 0.31600211351948715. Rolling Val Loss Avg: 0.08958016115206259. Elapsed Time: 411m 38s. accuracy: 14\n",
      "Iteration: 124000. Loss: 0.19492146372795105. Rolling Loss Avg: 0.31520798619183116. Rolling Val Loss Avg: 0.13312872692390723. Elapsed Time: 413m 17s. accuracy: 17\n",
      "Iteration: 124500. Loss: 0.06438463926315308. Rolling Loss Avg: 0.3144231588399428. Rolling Val Loss Avg: 0.10757243246943862. Elapsed Time: 414m 56s. accuracy: 15\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "column_names = ['iteration', 'loss', 'avg_loss', 'avg_val_loss', 'time', 'accuracy']\n",
    "logging_df = pd.DataFrame(columns=column_names)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=150, factor=0.1, min_lr=1e-8)  # Using ReduceLROnPlateau schedule\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (samples, labels) in enumerate(train_loader):\n",
    "                \n",
    "        samples = samples.to(device)\n",
    "        samples = samples.requires_grad_()\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # print(labels.dtype)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(samples)\n",
    "        \n",
    "        \n",
    "        # torch.tensor(asd).round()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "                \n",
    "        all_losses.append(loss.item())\n",
    "                \n",
    "        if i % 500 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            inner_iter = 0\n",
    "            \n",
    "            val_criterion = nn.CrossEntropyLoss()\n",
    "            val_all_losses = []\n",
    "            \n",
    "            for j, (val_samples, val_labels) in enumerate(val_loader):   \n",
    "                val_samples = val_samples.to(device)\n",
    "                val_labels = val_labels.to(device)\n",
    "                val_outputs = model(val_samples)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += val_labels.size(0)\n",
    "                val_loss = val_criterion(val_outputs, val_labels)\n",
    "                val_all_losses.append(val_loss.item())\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == val_labels.cpu()).sum()\n",
    "                else:\n",
    "                    correct += (predicted == val_labels).sum()\n",
    "                if j > min(i, 25):\n",
    "                    break\n",
    "            \n",
    "            scheduler.step(np.mean(val_all_losses))\n",
    "            accuracy = 100 * correct / total\n",
    "            elapsed_time = timeSince(start)\n",
    "            # i_row = [i, loss.item(), accuracy, elapsed_time]\n",
    "            i_row = [i, loss.item(), np.mean(all_losses), np.mean(val_all_losses), elapsed_time, accuracy]\n",
    "            #logging_df.loc[i] = i_row\n",
    "            #logging_df.to_csv(dir_path + 'logs/iterations_metrics.csv')\n",
    "            if i % 5000 == 0:\n",
    "                torch.save(model.state_dict(), 'model_' + str(i) + '.pt')\n",
    "                        \n",
    "            print('Iteration: {}. Loss: {}. Rolling Loss Avg: {}. Rolling Val Loss Avg: {}. Elapsed Time: {}. accuracy: {}'\n",
    "                  .format(i, loss.item(), np.mean(all_losses), np.mean(val_all_losses), elapsed_time, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_may_25.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "A8tZZV4V4Jpr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faa18fa4ed0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KdV0P05r5ViT",
    "outputId": "8bca7466-c250-497c-e4f7-156df4a1e4b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(1, 500, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=500, out_features=11, bias=True)\n",
       "  (linear): Linear(in_features=500, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WQ-3vs7i4Jqq"
   },
   "outputs": [],
   "source": [
    "test_dataset = IonDataset(\"/kaggle/input/no-drift-data/kaggle_data/test_no_drift.csv\", seq_dim, for_training=False, train_avg=train_val_dataset.avg, train_std=train_val_dataset.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "e6ySkoSt4Jqw"
   },
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "TJ4PNra34Jq0"
   },
   "outputs": [],
   "source": [
    "test_p = np.zeros(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "caDTJ1YeqXsu"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/kaggle/input/no-drift-data/kaggle_data/sample_submission.csv\", dtype={'time': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "KEC-wDZw1NGC"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "CCkjRLDp4twq",
    "outputId": "dde87c13-72d1-401b-b0db-91427ab169c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. Elapsed Time: 0m 0s\n",
      "Iteration: 1000. Elapsed Time: 1m 45s\n",
      "Iteration: 2000. Elapsed Time: 3m 24s\n",
      "Iteration: 3000. Elapsed Time: 5m 2s\n",
      "Iteration: 4000. Elapsed Time: 6m 40s\n",
      "Iteration: 5000. Elapsed Time: 8m 18s\n",
      "Iteration: 6000. Elapsed Time: 9m 56s\n",
      "Iteration: 7000. Elapsed Time: 11m 35s\n",
      "Iteration: 8000. Elapsed Time: 13m 13s\n",
      "Iteration: 9000. Elapsed Time: 14m 51s\n",
      "Iteration: 10000. Elapsed Time: 16m 29s\n",
      "Iteration: 11000. Elapsed Time: 18m 12s\n",
      "Iteration: 12000. Elapsed Time: 19m 50s\n",
      "Iteration: 13000. Elapsed Time: 21m 28s\n",
      "Iteration: 14000. Elapsed Time: 23m 5s\n",
      "Iteration: 15000. Elapsed Time: 24m 43s\n",
      "Iteration: 16000. Elapsed Time: 26m 20s\n",
      "Iteration: 17000. Elapsed Time: 27m 57s\n",
      "Iteration: 18000. Elapsed Time: 29m 35s\n",
      "Iteration: 19000. Elapsed Time: 31m 12s\n",
      "Iteration: 20000. Elapsed Time: 32m 50s\n",
      "Iteration: 21000. Elapsed Time: 34m 32s\n",
      "Iteration: 22000. Elapsed Time: 36m 9s\n",
      "Iteration: 23000. Elapsed Time: 37m 46s\n",
      "Iteration: 24000. Elapsed Time: 39m 23s\n",
      "Iteration: 25000. Elapsed Time: 41m 1s\n",
      "Iteration: 26000. Elapsed Time: 42m 39s\n",
      "Iteration: 27000. Elapsed Time: 44m 17s\n",
      "Iteration: 28000. Elapsed Time: 45m 54s\n",
      "Iteration: 29000. Elapsed Time: 47m 31s\n",
      "Iteration: 30000. Elapsed Time: 49m 8s\n",
      "Iteration: 31000. Elapsed Time: 50m 52s\n",
      "Iteration: 32000. Elapsed Time: 52m 31s\n",
      "Iteration: 33000. Elapsed Time: 54m 8s\n",
      "Iteration: 34000. Elapsed Time: 55m 47s\n",
      "Iteration: 35000. Elapsed Time: 57m 25s\n",
      "Iteration: 36000. Elapsed Time: 59m 3s\n",
      "Iteration: 37000. Elapsed Time: 60m 41s\n",
      "Iteration: 38000. Elapsed Time: 62m 19s\n",
      "Iteration: 39000. Elapsed Time: 63m 56s\n",
      "Iteration: 40000. Elapsed Time: 65m 35s\n",
      "Iteration: 41000. Elapsed Time: 67m 18s\n",
      "Iteration: 42000. Elapsed Time: 68m 56s\n",
      "Iteration: 43000. Elapsed Time: 70m 34s\n",
      "Iteration: 44000. Elapsed Time: 72m 12s\n",
      "Iteration: 45000. Elapsed Time: 73m 51s\n",
      "Iteration: 46000. Elapsed Time: 75m 30s\n",
      "Iteration: 47000. Elapsed Time: 77m 11s\n",
      "Iteration: 48000. Elapsed Time: 78m 53s\n",
      "Iteration: 49000. Elapsed Time: 80m 35s\n",
      "Iteration: 50000. Elapsed Time: 82m 17s\n",
      "Iteration: 51000. Elapsed Time: 84m 3s\n",
      "Iteration: 52000. Elapsed Time: 85m 45s\n",
      "Iteration: 53000. Elapsed Time: 87m 26s\n",
      "Iteration: 54000. Elapsed Time: 89m 6s\n",
      "Iteration: 55000. Elapsed Time: 90m 44s\n",
      "Iteration: 56000. Elapsed Time: 92m 23s\n",
      "Iteration: 57000. Elapsed Time: 94m 2s\n",
      "Iteration: 58000. Elapsed Time: 95m 42s\n",
      "Iteration: 59000. Elapsed Time: 97m 23s\n",
      "Iteration: 60000. Elapsed Time: 99m 2s\n",
      "Iteration: 61000. Elapsed Time: 100m 50s\n",
      "Iteration: 62000. Elapsed Time: 102m 32s\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, (samples) in enumerate(test_loader):\n",
    "        samples = samples.to(device)\n",
    "        outputs = model(samples).detach()\n",
    "        pred = F.softmax(outputs, 1).detach().cpu().numpy().argmax(axis=-1)\n",
    "        test_p[i * batch_size: (i+1) * batch_size] = pred\n",
    "        if i % 1000 == 0:\n",
    "            elapsed_time = timeSince(start)\n",
    "            print('Iteration: {}. Elapsed Time: {}'\n",
    "                  .format(i, elapsed_time))\n",
    "            if i % 10000 == 0:\n",
    "                submission.open_channels = np.array(test_p, np.int)\n",
    "                submission.to_csv('submission_may_25_' + str(i) + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QaV6qdfE2BjM"
   },
   "outputs": [],
   "source": [
    "submission.open_channels = np.array(test_p, np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "lw7spFip2FO9"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission_may_25.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faa19ffafd0>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(submission['open_channels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
